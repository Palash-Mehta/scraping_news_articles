{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from selenium import webdriver\n",
    "from urllib.request import urlopen\n",
    "import sys, time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_from_jacobinmag(domain_name):\n",
    "    article_urls = []\n",
    "    article_title = []\n",
    "    article_text = []\n",
    "    url_name = {}\n",
    "    # Extract News URL's from 3 pages\n",
    "    for i in range(1,4): \n",
    "        try:\n",
    "            url_name['url_' + str(i)] = \"https://\" + domain_name + \"/search?query=media%20censorship&page=\" + str(i)\n",
    "            uClient = urlopen(url_name['url_' + str(i)])\n",
    "            read_html = uClient.read()\n",
    "            uClient.close()\n",
    "        except:\n",
    "            print(\"Cannot connect to \" + domain_name)\n",
    "        page_soup = soup(read_html, \"html.parser\")\n",
    "        text_sections = page_soup.find(\"div\", {\"class\" : \"ar-mn__articles\"}).find_all(\"a\")\n",
    "        article_urls_containing_duplicate = []\n",
    "        for text in text_sections:\n",
    "            if \"/2020/\" in text.get(\"href\") or \"/2019/\" in text.get(\"href\"):\n",
    "                t = 'https://' + domain_name + text.get(\"href\")\n",
    "                article_urls_containing_duplicate.append(t)\n",
    "        for i, url in enumerate(article_urls_containing_duplicate):\n",
    "            if i % 2 != 0:\n",
    "                article_urls.append(url)\n",
    "\n",
    "    # We have the URL's, now we extract news from each URL\n",
    "    for i, url in enumerate(article_urls):\n",
    "        try:\n",
    "            uClient = urlopen(url)\n",
    "            read_html = uClient.read()\n",
    "            uClient.close()\n",
    "        except:\n",
    "            print(\"Cannot connect to \" + domain_name)\n",
    "        page_soup = soup(read_html, \"html.parser\")\n",
    "        title = page_soup.find(\"h1\", {\"class\" : \"po-hr-cn__title\"})\n",
    "        article_title.append(title.text)\n",
    "        paragraphs = page_soup.find(\"div\", {\"id\" : \"post-content\"}).find_all(\"p\")\n",
    "        intermediate_article_text = []\n",
    "        for paragraph in paragraphs:\n",
    "            intermediate_article_text.append(paragraph.text)\n",
    "        article = \"\".join(intermediate_article_text)\n",
    "        article_text.append(article)\n",
    "        \n",
    "    # Create a Dataframe\n",
    "    dicti = {\"Article_Heading\" : article_title, \"Article_Content\" : article_text, \"Source\" : article_urls}\n",
    "    df = pd.DataFrame(dicti)\n",
    "    print(\"Extracted articles from Jacobinmag.com\")\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_from_nytimes(domain_name):\n",
    "    article_urls = []\n",
    "    article_title = []\n",
    "    article_text = []\n",
    "    \n",
    "    url_2 = \"https://\" + domain_name + \"/search?query=media+censorship\"\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(url_2)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"Cannot connect to \" + domain_name)\n",
    "        \n",
    "    element = driver.find_element_by_xpath(\"//button[@data-testid='search-show-more-button']\")\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "    element.click()\n",
    "    time.sleep(10)\n",
    "    \n",
    "    page_soup = soup(driver.page_source, \"html.parser\")\n",
    "    text_sections = page_soup.find(\"div\", {\"class\" : \"css-46b038\"}).find_all(\"a\")\n",
    "\n",
    "    for text in text_sections:\n",
    "        if \"/2020/\" in text.get(\"href\") or \"/2019/\" in text.get(\"href\"):\n",
    "            t = 'https://' + domain_name + text.get(\"href\")\n",
    "            article_urls.append(t)\n",
    "\n",
    "    for url in article_urls:\n",
    "        try:\n",
    "            uClient = urlopen(url)\n",
    "            read_html = uClient.read()\n",
    "            uClient.close()\n",
    "        except:\n",
    "            print(\"Cannot connect to \" + domain_name)\n",
    "\n",
    "        page_soup = soup(read_html, \"html.parser\")\n",
    "        title = page_soup.find(\"h1\", {\"itemprop\": \"headline\"})\n",
    "        article_title.append(title.text)\n",
    "        paragraphs = page_soup.find(\"section\", {\"class\" : \"meteredContent css-1r7ky0e\"}).findAll(\"p\")\n",
    "        intermediate_article_text = []\n",
    "        for paragraph in paragraphs:\n",
    "            intermediate_article_text.append(paragraph.text)\n",
    "        article = \"\".join(intermediate_article_text)\n",
    "        article_text.append(article)\n",
    "\n",
    "    dicti = {\"Article_Heading\":article_title, \"Article_Content\":article_text, \"Source\":article_urls}\n",
    "    df = pd.DataFrame(dicti)\n",
    "    print(\"Extracted articles from nytimes.com\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_from_washington_post(domain_name):\n",
    "    url_3 = [\"https://\"+ domain_name+ \"/newssearch/?query=media%20censorship&sort=Relevance&datefilter=All%20Since%202005\",\n",
    "\"https://\"+ domain_name + \"/newssearch/?query=media%20censorship&sort=Relevance&datefilter=All%20Since%202005&spellcheck&startat=20#top\"]\n",
    "    article_urls = []\n",
    "    article_title = []\n",
    "    article_text = []\n",
    "    \n",
    "    for i,url in enumerate(url_3):\n",
    "        try:\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Cannot connect to \" + domain_name)\n",
    "\n",
    "        elements = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "        for i, elem in enumerate(elements):\n",
    "            if '/2020/' in elem.get_attribute(\"href\") and (i % 2) !=0:\n",
    "                article_urls.append(elem.get_attribute(\"href\"))\n",
    "        article_urls = article_urls[:-2] \n",
    "\n",
    "    for i,url in enumerate(article_urls):\n",
    "        try:\n",
    "            uClient = urlopen(url)\n",
    "            read_html = uClient.read()\n",
    "            uClient.close()\n",
    "        except:\n",
    "            print(\"Caanot connect to \" + domain_name)\n",
    "        page_soup = soup(read_html, \"html.parser\")\n",
    "        # title = page_soup.find(\"h1\", {\"data-qa\": \"headline\"})\n",
    "        title = page_soup.find(\"h1\")\n",
    "        article_title.append(title.text)\n",
    "        paragraphs = page_soup.find_all(\"p\", {\"class\" : \"font--body font-copy gray-darkest ma-0 pb-md\"})\n",
    "        intermediate_article_text = []\n",
    "        for paragraph in paragraphs:\n",
    "            intermediate_article_text.append(paragraph.text)\n",
    "        article = \"\".join(intermediate_article_text)\n",
    "        article_text.append(article)\n",
    "    \n",
    "    dicti = {\"Article_Heading\":article_title, \"Article_Content\":article_text, \"Source\":article_urls}\n",
    "    df = pd.DataFrame(dicti)\n",
    "    print(\"Extracted articles from washingtonpost.com\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    domain_name_1 = \"jacobinmag.com\"\n",
    "    domain_name_2 = \"nytimes.com\"\n",
    "    domain_name_3 = \"washingtonpost.com\"\n",
    "    df1 = extract_news_from_jacobinmag(domain_name_1)\n",
    "    df2 = extract_news_from_nytimes(domain_name_2)\n",
    "    df3 = extract_news_from_washington_post(domain_name_3)\n",
    "    final_df = pd.concat([df1, df2, df3], ignore_index = True)\n",
    "    final_df.to_excel('news_articles.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted articles from Jacobinmag.com\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"DONE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
